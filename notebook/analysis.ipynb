{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53a5abcc-97a1-4773-9270-b108595a90a8",
   "metadata": {},
   "source": [
    "# Scientometric Analysis for DoD Lean Six Sigma Research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a81ffab-91fc-431b-ac3f-f9f27404be2c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30dbc68e-bb95-4e94-9a3e-11ee2096cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import openpyxl\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "from openpyxl.utils import get_column_letter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4adc5831-6bdd-4403-854e-91a5b87d257d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Set up the environment\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "# Create results directory\n",
    "RESULTS_PATH = \"results\"\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_excel(\"sources.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ab8aedd-842a-4474-bbc6-4ad0c94f8fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK resources (uncomment if needed)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bb25d4-5e18-423d-b822-ac5e5d1a13aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50e5bf03-67e3-49e3-8406-eb32d0b05edc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_publication_trend(df):\n",
    "    \"\"\"Create publication trend line figure\"\"\"\n",
    "    print(\"Analyzing publication trends...\")\n",
    "    \n",
    "    # Group by year and count publications\n",
    "    publications_by_year = df['year'].value_counts().sort_index()\n",
    "    years = list(publications_by_year.index)\n",
    "    publication_counts = list(publications_by_year.values)\n",
    "    \n",
    "    # Create publication trend visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(years, publication_counts, color='steelblue', alpha=0.7)\n",
    "    plt.plot(years, publication_counts, color='red', marker='o', linewidth=2)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(years, publication_counts, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(years, p(years), \"r--\", alpha=0.8, linewidth=1.5)\n",
    "    \n",
    "    # Add 3-year moving average\n",
    "    if len(years) >= 3:\n",
    "        moving_avg = []\n",
    "        for i in range(len(publication_counts) - 2):\n",
    "            avg = np.mean(publication_counts[i:i+3])\n",
    "            moving_avg.append(avg)\n",
    "        \n",
    "        plt.plot(years[1:-1], moving_avg, color='green', marker='x', \n",
    "                 linestyle='-.', linewidth=1.5, label='3-Year Moving Average')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.title('Publication Trend of DoD Lean Six Sigma Research', fontsize=16)\n",
    "    plt.xlabel('Year', fontsize=14)\n",
    "    plt.ylabel('Number of Publications', fontsize=14)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(years, rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_PATH}/publication_trend.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Export data to CSV\n",
    "    trend_df = pd.DataFrame({'Year': years, 'Publications': publication_counts})\n",
    "    trend_df.to_csv(f'{RESULTS_PATH}/publication_trend_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50f43fae-2e2f-4660-bbbf-eebdaf298d8a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text for NLP analysis\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and short words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    custom_stopwords = {'department', 'defense', 'dod', 'military', 'study', 'research', \n",
    "                      'paper', 'analysis', 'approach', 'method', 'methodology',\n",
    "                      'data', 'result', 'thesis', 'dissertation', 'university'}\n",
    "    stop_words.update(custom_stopwords)\n",
    "    \n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d33dcbf4-09a3-4fd4-b964-18523fe43972",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_document_types(df):\n",
    "    \"\"\"Analyze document types in the dataset using automatic categorization\"\"\"\n",
    "    print(\"Analyzing document types...\")\n",
    "    \n",
    "    # Check if we have the document type column\n",
    "    if 'documentType' not in df.columns:\n",
    "        print(\"No 'documentType' column found. Checking alternate column names...\")\n",
    "        \n",
    "        # Try alternate column names\n",
    "        alt_names = ['DocumentType', 'document_type', 'articleType', 'ArticleType', 'docType', 'DocType']\n",
    "        found = False\n",
    "        \n",
    "        for col in alt_names:\n",
    "            if col in df.columns:\n",
    "                print(f\"Found document type information in column '{col}'\")\n",
    "                df['documentType'] = df[col]\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            print(\"No document type information found in the dataset.\")\n",
    "            return None\n",
    "    \n",
    "    # Clean document types (handle missing values)\n",
    "    df['documentType'] = df['documentType'].fillna('Unknown')\n",
    "    \n",
    "    # Extract unique document types automatically\n",
    "    unique_doc_types = set()\n",
    "    \n",
    "    # Handle compound document types (e.g., \"Journal Article, Case Study\")\n",
    "    all_doc_types = []\n",
    "    for doc_type in df['documentType']:\n",
    "        if pd.isna(doc_type) or doc_type == '':\n",
    "            continue\n",
    "            \n",
    "        # Split by comma if it contains multiple types\n",
    "        if ',' in doc_type:\n",
    "            types = [t.strip() for t in doc_type.split(',')]\n",
    "            for t in types:\n",
    "                if t:  # Only add non-empty strings\n",
    "                    unique_doc_types.add(t)\n",
    "                    all_doc_types.append(t)\n",
    "        else:\n",
    "            if doc_type.strip():  # Only add non-empty strings\n",
    "                unique_doc_types.add(doc_type.strip())\n",
    "                all_doc_types.append(doc_type.strip())\n",
    "    \n",
    "    print(f\"Found {len(unique_doc_types)} unique document types:\")\n",
    "    for doc_type in sorted(unique_doc_types):\n",
    "        print(f\"  - {doc_type}\")\n",
    "    \n",
    "    # Count document types\n",
    "    doc_type_counts = pd.Series(all_doc_types).value_counts()\n",
    "    \n",
    "    # Remove 'Unknown' if it's a small percentage\n",
    "    if 'Unknown' in doc_type_counts and doc_type_counts['Unknown'] / doc_type_counts.sum() < 0.05:\n",
    "        doc_type_counts = doc_type_counts.drop('Unknown')\n",
    "    \n",
    "    # Calculate percentage distribution\n",
    "    doc_type_percent = (doc_type_counts / doc_type_counts.sum() * 100).round(1)\n",
    "    \n",
    "    # Function to assign primary document type (for papers with multiple types)\n",
    "    def get_primary_doc_type(doc_type):\n",
    "        if pd.isna(doc_type) or doc_type == '':\n",
    "            return 'Unknown'\n",
    "            \n",
    "        # If it has multiple types, take the first one\n",
    "        if ',' in doc_type:\n",
    "            return doc_type.split(',')[0].strip()\n",
    "        \n",
    "        return doc_type.strip()\n",
    "    \n",
    "    # Assign primary document type to each paper\n",
    "    df['primary_doc_type'] = df['documentType'].apply(get_primary_doc_type)\n",
    "    \n",
    "    # Create visualizations only if we have enough unique types\n",
    "    if len(doc_type_counts) > 0:\n",
    "        # Create pie chart - limit to top 10 categories if there are too many\n",
    "        if len(doc_type_counts) > 10:\n",
    "            print(f\"Limiting pie chart to top 10 of {len(doc_type_counts)} document types\")\n",
    "            plot_counts = doc_type_counts.nlargest(9)\n",
    "            # Add an \"Other\" category for the rest\n",
    "            other_count = doc_type_counts[~doc_type_counts.index.isin(plot_counts.index)].sum()\n",
    "            plot_counts['Other'] = other_count\n",
    "        else:\n",
    "            plot_counts = doc_type_counts\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        wedges, texts, autotexts = plt.pie(\n",
    "            plot_counts, \n",
    "            labels=plot_counts.index, \n",
    "            autopct='%1.1f%%',\n",
    "            shadow=False, \n",
    "            startangle=90, \n",
    "            explode=[0.05] * len(plot_counts),\n",
    "            colors=plt.cm.tab10(np.linspace(0, 1, len(plot_counts)))\n",
    "        )\n",
    "        \n",
    "        # Enhance text visibility\n",
    "        for text in texts:\n",
    "            text.set_fontsize(11)\n",
    "        for autotext in autotexts:\n",
    "            autotext.set_fontsize(9)\n",
    "            autotext.set_color('white')\n",
    "            autotext.set_fontweight('bold')\n",
    "        \n",
    "        plt.title('Distribution of Document Types in DoD Lean Six Sigma Literature', fontsize=16)\n",
    "        plt.axis('equal')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{RESULTS_PATH}/document_type_distribution_pie.png', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Create bar chart with count and percentage\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # For bar chart, show all categories but limit if there are too many\n",
    "        if len(doc_type_counts) > 15:\n",
    "            plot_counts = doc_type_counts.nlargest(15)\n",
    "            plt.title('Top 15 Document Types in DoD Lean Six Sigma Literature', fontsize=16)\n",
    "        else:\n",
    "            plot_counts = doc_type_counts\n",
    "            plt.title('Distribution of Document Types in DoD Lean Six Sigma Literature', fontsize=16)\n",
    "        \n",
    "        bars = plt.bar(\n",
    "            plot_counts.index,\n",
    "            plot_counts.values,\n",
    "            color=plt.cm.tab10(np.linspace(0, 1, len(plot_counts)))\n",
    "        )\n",
    "        \n",
    "        plt.xlabel('Document Type', fontsize=14)\n",
    "        plt.ylabel('Number of Documents', fontsize=14)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add data labels with count and percentage\n",
    "        for i, (count, percent) in enumerate(zip(plot_counts, \n",
    "                                                [doc_type_percent[idx] for idx in plot_counts.index])):\n",
    "            plt.text(\n",
    "                i, \n",
    "                count + 0.5, \n",
    "                f\"{count} ({percent}%)\", \n",
    "                ha='center', \n",
    "                fontsize=10,\n",
    "                fontweight='bold'\n",
    "            )\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{RESULTS_PATH}/document_type_distribution_bar.png', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Document type trends over time\n",
    "        try:\n",
    "            # Group by year and document type (using primary doc type for trend analysis)\n",
    "            yearly_doc_type = df.groupby(['year', 'primary_doc_type']).size().unstack(fill_value=0)\n",
    "            \n",
    "            # Only keep the most common document types for clarity\n",
    "            if len(doc_type_counts) > 5:\n",
    "                top_doc_types = doc_type_counts.nlargest(5).index.tolist()\n",
    "                yearly_doc_type_filtered = yearly_doc_type[yearly_doc_type.columns.intersection(top_doc_types)]\n",
    "                \n",
    "                # If we don't have all top document types in the filtered DataFrame, add them with zeros\n",
    "                for doc_type in top_doc_types:\n",
    "                    if doc_type not in yearly_doc_type_filtered.columns:\n",
    "                        yearly_doc_type_filtered[doc_type] = 0\n",
    "                \n",
    "                # Ensure we only have the top document types (no more, no less)\n",
    "                yearly_doc_type_filtered = yearly_doc_type_filtered[\n",
    "                    [col for col in top_doc_types if col in yearly_doc_type_filtered.columns]\n",
    "                ]\n",
    "            else:\n",
    "                yearly_doc_type_filtered = yearly_doc_type\n",
    "            \n",
    "            if not yearly_doc_type_filtered.empty:\n",
    "                # Plot document type trends over time\n",
    "                plt.figure(figsize=(14, 8))\n",
    "                ax = yearly_doc_type_filtered.plot(\n",
    "                    kind='line', \n",
    "                    marker='o',\n",
    "                    linewidth=2,\n",
    "                    markersize=6,\n",
    "                    ax=plt.gca()\n",
    "                )\n",
    "                \n",
    "                plt.title('Document Types in DoD Lean Six Sigma Research Over Time', fontsize=16)\n",
    "                plt.xlabel('Year', fontsize=14)\n",
    "                plt.ylabel('Number of Publications', fontsize=14)\n",
    "                plt.grid(True, linestyle='--', alpha=0.7)\n",
    "                plt.legend(title='Document Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'{RESULTS_PATH}/document_type_trends.png', dpi=300)\n",
    "                plt.close()\n",
    "                \n",
    "                # Create stacked area chart\n",
    "                plt.figure(figsize=(14, 8))\n",
    "                yearly_doc_type_filtered.plot.area(stacked=True, alpha=0.7, ax=plt.gca())\n",
    "                plt.title('Cumulative Document Types in DoD Lean Six Sigma Research', fontsize=16)\n",
    "                plt.xlabel('Year', fontsize=14)\n",
    "                plt.ylabel('Number of Publications', fontsize=14)\n",
    "                plt.grid(True, linestyle='--', alpha=0.7)\n",
    "                plt.legend(title='Document Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'{RESULTS_PATH}/document_type_cumulative.png', dpi=300)\n",
    "                plt.close()\n",
    "                \n",
    "                # Calculate proportional representation over time (percentage)\n",
    "                yearly_doc_type_pct = yearly_doc_type_filtered.div(yearly_doc_type_filtered.sum(axis=1), axis=0) * 100\n",
    "                \n",
    "                plt.figure(figsize=(14, 8))\n",
    "                yearly_doc_type_pct.plot.area(stacked=True, alpha=0.7, ax=plt.gca())\n",
    "                plt.title('Proportion of Document Types in DoD Lean Six Sigma Research Over Time', fontsize=16)\n",
    "                plt.xlabel('Year', fontsize=14)\n",
    "                plt.ylabel('Percentage of Publications', fontsize=14)\n",
    "                plt.grid(True, linestyle='--', alpha=0.7)\n",
    "                plt.legend(title='Document Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'{RESULTS_PATH}/document_type_percentage.png', dpi=300)\n",
    "                plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating time trend visualizations: {e}\")\n",
    "    \n",
    "    # Export document type data\n",
    "    doc_type_df = pd.DataFrame({\n",
    "        'Document_Type': doc_type_counts.index, \n",
    "        'Count': doc_type_counts.values,\n",
    "        'Percentage': doc_type_percent.values\n",
    "    })\n",
    "    doc_type_df.to_csv(f'{RESULTS_PATH}/document_type_distribution.csv', index=False)\n",
    "    \n",
    "    try:\n",
    "        if 'yearly_doc_type' in locals() and not yearly_doc_type.empty:\n",
    "            yearly_doc_type.to_csv(f'{RESULTS_PATH}/document_type_yearly.csv')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(f\"Document type analysis complete. Found {len(doc_type_counts)} document types.\")\n",
    "    \n",
    "    return doc_type_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b1be686-b8ce-4ab2-90df-6e9585cd8cf3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_keywords(df):\n",
    "    \"\"\"Analyze keywords: frequency and network\"\"\"\n",
    "    print(\"Analyzing keywords...\")\n",
    "    \n",
    "    # Preprocess abstracts\n",
    "    df['processed_abstract'] = df['Abstract'].apply(preprocess_text)\n",
    "    \n",
    "    # Combine all processed abstracts for word frequency analysis\n",
    "    all_text = ' '.join(df['processed_abstract'].dropna())\n",
    "    words = word_tokenize(all_text)\n",
    "    word_freq = Counter(words)\n",
    "    most_common_words = word_freq.most_common(30)\n",
    "    \n",
    "    # Plot word frequency\n",
    "    words, counts = zip(*most_common_words)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    y_pos = np.arange(len(words))\n",
    "    plt.barh(y_pos, counts, align='center', color='skyblue')\n",
    "    plt.yticks(y_pos, words)\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.title('Most Common Keywords in DoD Lean Six Sigma Literature', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_PATH}/keyword_frequency.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Create word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white',\n",
    "                          max_words=100, contour_width=3, contour_color='steelblue')\n",
    "    wordcloud.generate(all_text)\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_PATH}/wordcloud.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Keyword co-occurrence analysis\n",
    "    def extract_keywords(text):\n",
    "        \"\"\"Extract keywords from text\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return []\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords and short words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        custom_stopwords = {'department', 'defense', 'dod', 'military', 'study', 'research', \n",
    "                          'paper', 'analysis', 'approach', 'method', 'methodology'}\n",
    "        stop_words.update(custom_stopwords)\n",
    "        \n",
    "        tokens = [word for word in tokens if word.isalpha() and word not in stop_words and len(word) > 2]\n",
    "        return tokens\n",
    "    \n",
    "    # Extract keywords from each abstract\n",
    "    df['keywords'] = df['Abstract'].apply(extract_keywords)\n",
    "    \n",
    "    # Create co-occurrence matrix\n",
    "    keywords_list = []\n",
    "    for keywords in df['keywords']:\n",
    "        keywords_list.extend(keywords)\n",
    "    \n",
    "    # Get the most common keywords - limiting to top 30 instead of 50 for clearer visualization\n",
    "    most_common = Counter(keywords_list).most_common(30)\n",
    "    top_keywords = [word for word, count in most_common]\n",
    "    \n",
    "    # Create co-occurrence matrix\n",
    "    co_occurrence = np.zeros((len(top_keywords), len(top_keywords)))\n",
    "    for keywords in df['keywords']:\n",
    "        for i, keyword1 in enumerate(top_keywords):\n",
    "            if keyword1 in keywords:\n",
    "                for j, keyword2 in enumerate(top_keywords):\n",
    "                    if keyword2 in keywords:\n",
    "                        co_occurrence[i, j] += 1\n",
    "    \n",
    "    # Create network but with stronger filtering for a less dense graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes first\n",
    "    for i, keyword1 in enumerate(top_keywords):\n",
    "        # Only add nodes with sufficient frequency\n",
    "        if most_common[i][1] > 5:  # Only include keywords that appear more than 5 times\n",
    "            G.add_node(keyword1, size=most_common[i][1])\n",
    "    \n",
    "    # Now add edges, but with a higher threshold\n",
    "    threshold = max(5, np.mean(co_occurrence) + 0.5 * np.std(co_occurrence))  # Dynamic threshold\n",
    "    print(f\"Co-occurrence threshold: {threshold}\")\n",
    "    \n",
    "    for i, keyword1 in enumerate(top_keywords):\n",
    "        if keyword1 not in G:\n",
    "            continue\n",
    "        for j, keyword2 in enumerate(top_keywords):\n",
    "            if i != j and keyword2 in G and co_occurrence[i, j] > threshold:\n",
    "                G.add_edge(keyword1, keyword2, weight=co_occurrence[i, j])\n",
    "    \n",
    "    # Remove isolated nodes (no connections)\n",
    "    isolated_nodes = [node for node, degree in dict(G.degree()).items() if degree == 0]\n",
    "    G.remove_nodes_from(isolated_nodes)\n",
    "    \n",
    "    # Print network summary\n",
    "    print(f\"Network contains {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "    \n",
    "    # Check if we have a graph to visualize\n",
    "    if G.number_of_nodes() == 0:\n",
    "        print(\"No nodes in graph after filtering. Try reducing the threshold.\")\n",
    "        return\n",
    "    \n",
    "    # Create main network visualization\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    \n",
    "    # Try different layout algorithms for better visualization\n",
    "    try:\n",
    "        # Try community-based layout if we have enough nodes\n",
    "        if G.number_of_nodes() > 5:\n",
    "            pos = nx.spring_layout(G, k=0.4, iterations=100, seed=42)\n",
    "        else:\n",
    "            pos = nx.circular_layout(G)\n",
    "    except:\n",
    "        # Fallback to basic layout\n",
    "        pos = nx.spring_layout(G, k=0.3, seed=42)\n",
    "    \n",
    "    # Calculate node sizes based on frequency, with normalization\n",
    "    max_node_size = 2000\n",
    "    min_node_size = 300\n",
    "    node_sizes = []\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        size = G.nodes[node]['size']\n",
    "        # Normalize to a reasonable size range\n",
    "        normalized_size = min_node_size + (size / max([G.nodes[n]['size'] for n in G.nodes()])) * (max_node_size - min_node_size)\n",
    "        node_sizes.append(normalized_size)\n",
    "    \n",
    "    # Calculate edge weights, normalized for better visualization\n",
    "    max_width = 5\n",
    "    min_width = 0.5\n",
    "    \n",
    "    if G.number_of_edges() > 0:\n",
    "        max_weight = max([G.edges[edge]['weight'] for edge in G.edges()])\n",
    "        edge_weights = [min_width + (G.edges[edge]['weight'] / max_weight) * (max_width - min_width) for edge in G.edges()]\n",
    "    else:\n",
    "        edge_weights = []\n",
    "    \n",
    "    # Draw the network with coloring based on centrality\n",
    "    # Calculate betweenness centrality\n",
    "    if G.number_of_nodes() > 1:\n",
    "        centrality = nx.betweenness_centrality(G)\n",
    "        centrality_values = [centrality[node] for node in G.nodes()]\n",
    "        \n",
    "        # Draw nodes with centrality-based colors\n",
    "        nodes = nx.draw_networkx_nodes(G, pos, \n",
    "                                       node_size=node_sizes, \n",
    "                                       node_color=centrality_values,\n",
    "                                       cmap=plt.cm.viridis, \n",
    "                                       alpha=0.8)\n",
    "        \n",
    "        # Add colorbar for centrality\n",
    "        plt.colorbar(nodes, label='Betweenness Centrality', shrink=0.8)\n",
    "    else:\n",
    "        # Simple drawing for small networks\n",
    "        nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='skyblue', alpha=0.8)\n",
    "    \n",
    "    # Draw edges with transparency for less visual clutter\n",
    "    nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.3, edge_color='gray')\n",
    "    \n",
    "    # Draw labels with appropriate font size - FIX: Use a single font size value\n",
    "    # Calculate the font size based on node importance, but as a number not a dictionary\n",
    "    base_font_size = 10\n",
    "    # Just use a fixed font size for all labels\n",
    "    nx.draw_networkx_labels(G, pos, font_size=base_font_size, font_family='sans-serif', font_weight='bold')\n",
    "    \n",
    "    plt.title('Keyword Co-occurrence Network (Filtered for Clarity)', fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_PATH}/keyword_network.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a simplified network visualization for better readability\n",
    "    # Keep only the top connections\n",
    "    G_simple = nx.Graph()\n",
    "    \n",
    "    # Add the top nodes\n",
    "    top_n_keywords = 15  # Limit to top 15 keywords\n",
    "    for i, (word, count) in enumerate(most_common[:top_n_keywords]):\n",
    "        G_simple.add_node(word, size=count)\n",
    "    \n",
    "    # Add only the strongest connections\n",
    "    high_threshold = max(10, np.percentile(co_occurrence[co_occurrence > 0], 90))  # Very high threshold\n",
    "    \n",
    "    for i, keyword1 in enumerate(top_keywords[:top_n_keywords]):\n",
    "        for j, keyword2 in enumerate(top_keywords[:top_n_keywords]):\n",
    "            if i < j and co_occurrence[i, j] > high_threshold:\n",
    "                G_simple.add_edge(keyword1, keyword2, weight=co_occurrence[i, j])\n",
    "    \n",
    "    # Create a simplified visualization\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    pos_simple = nx.spring_layout(G_simple, k=0.5, iterations=100, seed=42)\n",
    "    \n",
    "    # Node sizes simplified\n",
    "    node_sizes_simple = [G_simple.nodes[node]['size'] * 30 for node in G_simple.nodes()]\n",
    "    \n",
    "    # Draw simplified network\n",
    "    nx.draw_networkx_nodes(G_simple, pos_simple, node_size=node_sizes_simple, node_color='lightblue', alpha=0.8)\n",
    "    \n",
    "    # Draw edges with width based on weight\n",
    "    for u, v, d in G_simple.edges(data=True):\n",
    "        nx.draw_networkx_edges(G_simple, pos_simple, edgelist=[(u, v)], width=d['weight'] * 0.05, alpha=0.6)\n",
    "    \n",
    "    nx.draw_networkx_labels(G_simple, pos_simple, font_size=12, font_family='sans-serif', font_weight='bold')\n",
    "    \n",
    "    plt.title('Simplified Keyword Co-occurrence Network (Top Terms Only)', fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_PATH}/keyword_network_simplified.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Export keyword data\n",
    "    keyword_df = pd.DataFrame(most_common, columns=['Keyword', 'Frequency'])\n",
    "    keyword_df.to_csv(f'{RESULTS_PATH}/keyword_frequency_data.csv', index=False)\n",
    "    \n",
    "    # Export co-occurrence matrix for the most common keywords\n",
    "    cooc_df = pd.DataFrame(co_occurrence, index=top_keywords, columns=top_keywords)\n",
    "    cooc_df.to_csv(f'{RESULTS_PATH}/keyword_cooccurrence_matrix.csv')\n",
    "    \n",
    "    return keyword_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81f4c8a4-4f3a-423d-a010-2fbc4bc4c78f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_topics_lda(df):\n",
    "    \"\"\"Perform LDA topic modeling on abstracts\"\"\"\n",
    "    print(\"Performing LDA topic analysis...\")\n",
    "    \n",
    "    # Ensure we have processed abstracts\n",
    "    if 'processed_abstract' not in df.columns:\n",
    "        df['processed_abstract'] = df['Abstract'].apply(preprocess_text)\n",
    "    \n",
    "    # Create document-term matrix\n",
    "    print(\"Creating document-term matrix...\")\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000)\n",
    "    dtm = vectorizer.fit_transform(df['processed_abstract'])\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Perform LDA with 4 topics\n",
    "    print(\"Fitting LDA model with 4 topics...\")\n",
    "    lda_model = LatentDirichletAllocation(\n",
    "        n_components=4,\n",
    "        random_state=42,\n",
    "        max_iter=20,\n",
    "        learning_method='online'\n",
    "    )\n",
    "    \n",
    "    lda_output = lda_model.fit_transform(dtm)\n",
    "    \n",
    "    # Assign dominant topic to each document\n",
    "    dominant_topic = np.argmax(lda_output, axis=1)\n",
    "    df['dominant_topic'] = dominant_topic\n",
    "    \n",
    "    # Get top words for each topic\n",
    "    def get_top_words(model, feature_names, n_top_words):\n",
    "        topic_words = []\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            top_words_idx = topic.argsort()[:-n_top_words-1:-1]\n",
    "            top_words = [feature_names[i] for i in top_words_idx]\n",
    "            topic_words.append((topic_idx, top_words))\n",
    "        return topic_words\n",
    "    \n",
    "    top_words = get_top_words(lda_model, feature_names, 15)\n",
    "    \n",
    "    # Create topic labels based on top words\n",
    "    topic_labels = {}\n",
    "    for topic_idx, words in top_words:\n",
    "        topic_labels[topic_idx] = f\"Topic {topic_idx+1}: {', '.join(words[:3])}\"\n",
    "    \n",
    "    # Visualize topics with descriptive titles\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10), sharey=True)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot top words for each topic\n",
    "    for i, (topic_idx, words) in enumerate(top_words):\n",
    "        word_importance = [lda_model.components_[topic_idx][feature_names.tolist().index(word)] for word in words]\n",
    "        ax = axes[i]\n",
    "        y_pos = np.arange(len(words))\n",
    "        ax.barh(y_pos, word_importance, align='center')\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(words)\n",
    "        ax.invert_yaxis()\n",
    "        # Use descriptive title with top 3 words\n",
    "        ax.set_title(topic_labels[topic_idx], fontsize=12)\n",
    "        ax.set_xlabel('Importance')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_PATH}/lda_topics.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Create topic distribution visualization\n",
    "    topic_counts = df['dominant_topic'].value_counts().sort_index()\n",
    "    \n",
    "    # Create topic label list for x-axis (preserving order)\n",
    "    x_labels = [topic_labels[i] for i in range(len(topic_counts))]\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    bars = plt.bar(\n",
    "        range(len(topic_counts)), \n",
    "        topic_counts.values, \n",
    "        color=sns.color_palette(\"husl\", len(topic_counts))\n",
    "    )\n",
    "    \n",
    "    plt.title('Distribution of Documents Across LDA Topics', fontsize=16)\n",
    "    plt.xlabel('Topic', fontsize=14)\n",
    "    plt.ylabel('Number of Documents', fontsize=14)\n",
    "    plt.xticks(range(len(topic_counts)), x_labels, rotation=45, ha='right')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add data labels to the top of each bar\n",
    "    for i, v in enumerate(topic_counts.values):\n",
    "        plt.text(i, v + 0.5, str(v), ha='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_PATH}/topic_distribution.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Export document-topic assignment\n",
    "    topic_assignment = df[['Title', 'year', 'dominant_topic']].copy()\n",
    "    topic_assignment['topic_label'] = topic_assignment['dominant_topic'].map(\n",
    "        lambda x: topic_labels[x]\n",
    "    )\n",
    "    topic_assignment.to_csv(f'{RESULTS_PATH}/document_topic_assignment.csv', index=False)\n",
    "    \n",
    "    # Export top words for each topic\n",
    "    with open(f'{RESULTS_PATH}/topic_keywords.txt', 'w') as f:\n",
    "        for topic_idx, words in top_words:\n",
    "            f.write(f\"{topic_labels[topic_idx]}\\n\")\n",
    "            f.write(f\"All top words: {', '.join(words)}\\n\\n\")\n",
    "    \n",
    "    return topic_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b482cfe9-a904-473d-8a08-2f94301915ac",
   "metadata": {},
   "source": [
    "## Thematic Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af516b41-1232-4047-b20c-981b42df70e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 465 documents...\n",
      "Analyzing publication trends...\n",
      "Analyzing keywords...\n",
      "Co-occurrence threshold: 39.00901244310939\n",
      "Network contains 22 nodes and 69 edges\n",
      "Analyzing document types...\n",
      "Found 16 unique document types:\n",
      "  - Case Study\n",
      "  - Commentary\n",
      "  - Conference Proceedings\n",
      "  - Cover Story\n",
      "  - Dissertation/Thesis\n",
      "  - Feature\n",
      "  - General Information\n",
      "  - Interview\n",
      "  - Journal Article\n",
      "  - Letter\n",
      "  - News\n",
      "  - PERIODICAL\n",
      "  - Report\n",
      "  - Review\n",
      "  - article\n",
      "  - statistics\n",
      "Limiting pie chart to top 10 of 16 document types\n",
      "Document type analysis complete. Found 16 document types.\n",
      "Performing LDA topic analysis...\n",
      "Creating document-term matrix...\n",
      "Fitting LDA model with 4 topics...\n",
      "Analysis complete. Results saved to the 'results' directory.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Basic data cleaning\n",
    "df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "df = df.dropna(subset=['year', 'Abstract'])\n",
    "df['year'] = df['year'].astype(int)\n",
    "\n",
    "print(f\"Analyzing {len(df)} documents...\")\n",
    "\n",
    "# Run analyses\n",
    "analyze_publication_trend(df)\n",
    "analyze_keywords(df)\n",
    "analyze_document_types(df)\n",
    "analyze_topics_lda(df)\n",
    "\n",
    "print(\"Analysis complete. Results saved to the 'results' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd96d51d-31ee-4173-a9de-10da8dc309e4",
   "metadata": {},
   "source": [
    "## Theme Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8eacb27d-bb9b-43af-beff-f8446b036a4b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_thematic_categorization(df, themes):\n",
    "    \"\"\"\n",
    "    Categorize papers into predefined themes using keyword matching.\n",
    "    The function then analyzes distribution and trends over time for these themes.\n",
    "    Now outputs separate XLSX files for each theme.\n",
    "    \"\"\"\n",
    "    print(\"Categorizing papers into the three main themes...\")\n",
    "    \n",
    "    # Ensure we have processed abstracts\n",
    "    if 'processed_abstract' not in df.columns:\n",
    "        df['processed_abstract'] = df['Abstract'].apply(preprocess_text)\n",
    "    \n",
    "    \n",
    "    # Function to match document to themes\n",
    "    def categorize_document(abstract, title):\n",
    "        if pd.isna(abstract) and pd.isna(title):\n",
    "            return None\n",
    "        \n",
    "        # Combine and preprocess text\n",
    "        text = ''\n",
    "        if not pd.isna(abstract):\n",
    "            text += abstract.lower() + ' '\n",
    "        if not pd.isna(title):\n",
    "            text += title.lower()\n",
    "        \n",
    "        # Count matches for each theme\n",
    "        theme_scores = {}\n",
    "        for theme_name, keywords in themes.items():\n",
    "            # Count occurrences of each keyword\n",
    "            score = sum(1 for keyword in keywords if keyword in text)\n",
    "            theme_scores[theme_name] = score\n",
    "        \n",
    "        # Assign primary theme (highest score)\n",
    "        if max(theme_scores.values()) > 0:\n",
    "            primary_theme = max(theme_scores.items(), key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            primary_theme = \"Uncategorized\"\n",
    "            \n",
    "        return primary_theme, theme_scores\n",
    "    \n",
    "    # Apply categorization\n",
    "    categorization_results = df.apply(\n",
    "        lambda row: categorize_document(row['processed_abstract'], row['Title']), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Extract primary theme and scores\n",
    "    df['primary_theme'] = [result[0] if result else \"Uncategorized\" for result in categorization_results]\n",
    "    df['theme_scores'] = [result[1] if result else {} for result in categorization_results]\n",
    "    \n",
    "    # Count papers by theme\n",
    "    theme_counts = df['primary_theme'].value_counts()\n",
    "    theme_percent = (theme_counts / theme_counts.sum() * 100).round(1)\n",
    "    \n",
    "    print(\"\\nPapers by theme:\")\n",
    "    for theme, count in theme_counts.items():\n",
    "        print(f\"  {theme}: {count} papers ({theme_percent[theme]}%)\")\n",
    "    \n",
    "    # Create pie chart\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    colors = {'Management and Leadership': 'royalblue', \n",
    "              'Process Improvement': 'forestgreen',\n",
    "              'Continuous Learning': 'darkorange',\n",
    "              'Uncategorized': 'lightgray'}\n",
    "    \n",
    "    theme_colors = [colors.get(theme, 'lightgray') for theme in theme_counts.index]\n",
    "    wedges, texts, autotexts = plt.pie(\n",
    "        theme_counts, \n",
    "        labels=theme_counts.index, \n",
    "        autopct='%1.1f%%',\n",
    "        shadow=False, \n",
    "        startangle=90, \n",
    "        explode=[0.05] * len(theme_counts),\n",
    "        colors=theme_colors\n",
    "    )\n",
    "    \n",
    "    # Enhance text visibility\n",
    "    for text in texts:\n",
    "        text.set_fontsize(12)\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_fontsize(10)\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    \n",
    "    plt.title('Distribution of DoD Lean Six Sigma Papers by Theme', fontsize=16)\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_PATH}/theme_distribution_pie.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Create bar chart\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(\n",
    "        theme_counts.index,\n",
    "        theme_counts.values,\n",
    "        color=[colors.get(theme, 'lightgray') for theme in theme_counts.index]\n",
    "    )\n",
    "    \n",
    "    plt.title('Distribution of DoD Lean Six Sigma Papers by Theme', fontsize=16)\n",
    "    plt.xlabel('Theme', fontsize=14)\n",
    "    plt.ylabel('Number of Papers', fontsize=14)\n",
    "    plt.xticks(rotation=15, ha='right')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add data labels\n",
    "    for i, (count, percent) in enumerate(zip(theme_counts, theme_percent)):\n",
    "        plt.text(\n",
    "            i, \n",
    "            count + 0.5, \n",
    "            f\"{count} ({percent}%)\", \n",
    "            ha='center', \n",
    "            fontsize=10,\n",
    "            fontweight='bold'\n",
    "        )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_PATH}/theme_distribution_bar.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Generate word clouds for each theme\n",
    "    for theme in themes.keys():\n",
    "        if theme not in theme_counts.index:\n",
    "            continue\n",
    "            \n",
    "        # Select papers for this theme\n",
    "        theme_papers = df[df['primary_theme'] == theme]\n",
    "        \n",
    "        if len(theme_papers) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Combine all text for this theme\n",
    "        theme_text = ' '.join(theme_papers['processed_abstract'].dropna())\n",
    "        \n",
    "        if not theme_text.strip():\n",
    "            continue\n",
    "        \n",
    "        # Generate word cloud\n",
    "        wordcloud = WordCloud(\n",
    "            width=800, \n",
    "            height=400, \n",
    "            background_color='white',\n",
    "            max_words=100, \n",
    "            contour_width=3, \n",
    "            contour_color='steelblue'\n",
    "        ).generate(theme_text)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(f'Key Terms in {theme} Papers', fontsize=16)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{RESULTS_PATH}/wordcloud_{theme.replace(\" \", \"_\")}.png', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # Analyze theme trends over time\n",
    "    try:\n",
    "        # Group by year and theme\n",
    "        yearly_theme_counts = df.groupby(['year', 'primary_theme']).size().unstack(fill_value=0)\n",
    "        \n",
    "        # Plot theme trends\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        ax = yearly_theme_counts.plot(\n",
    "            kind='line', \n",
    "            marker='o',\n",
    "            linewidth=2,\n",
    "            markersize=6,\n",
    "            ax=plt.gca(),\n",
    "            color=[colors.get(theme, 'lightgray') for theme in yearly_theme_counts.columns]\n",
    "        )\n",
    "        \n",
    "        plt.title('Evolution of Research Themes Over Time', fontsize=16)\n",
    "        plt.xlabel('Year', fontsize=14)\n",
    "        plt.ylabel('Number of Publications', fontsize=14)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.legend(title='Theme', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{RESULTS_PATH}/theme_trends_over_time.png', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Create stacked area chart\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        yearly_theme_counts.plot.area(\n",
    "            stacked=True, \n",
    "            alpha=0.7, \n",
    "            ax=plt.gca(),\n",
    "            color=[colors.get(theme, 'lightgray') for theme in yearly_theme_counts.columns]\n",
    "        )\n",
    "        plt.title('Cumulative Growth of Research Themes', fontsize=16)\n",
    "        plt.xlabel('Year', fontsize=14)\n",
    "        plt.ylabel('Number of Publications', fontsize=14)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.legend(title='Theme', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{RESULTS_PATH}/theme_cumulative_growth.png', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Calculate percentage distribution over time\n",
    "        yearly_percentages = yearly_theme_counts.div(yearly_theme_counts.sum(axis=1), axis=0) * 100\n",
    "        \n",
    "        plt.figure(figsize=(14, 8))\n",
    "        yearly_percentages.plot.area(\n",
    "            stacked=True, \n",
    "            alpha=0.7, \n",
    "            ax=plt.gca(),\n",
    "            color=[colors.get(theme, 'lightgray') for theme in yearly_percentages.columns]\n",
    "        )\n",
    "        plt.title('Relative Proportion of Research Themes Over Time', fontsize=16)\n",
    "        plt.xlabel('Year', fontsize=14)\n",
    "        plt.ylabel('Percentage of Publications', fontsize=14)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.legend(title='Theme', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{RESULTS_PATH}/theme_percentage_over_time.png', dpi=300)\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in time trend analysis: {e}\")\n",
    "    \n",
    "    # Export data\n",
    "    # Save theme categorization\n",
    "    df[['Title', 'year', 'primary_theme']].to_csv(f'{RESULTS_PATH}/papers_by_theme.csv', index=False)\n",
    "    \n",
    "    # Save theme counts\n",
    "    theme_df = pd.DataFrame({\n",
    "        'Theme': theme_counts.index,\n",
    "        'Count': theme_counts.values,\n",
    "        'Percentage': theme_percent.values\n",
    "    })\n",
    "    theme_df.to_csv(f'{RESULTS_PATH}/theme_distribution.csv', index=False)\n",
    "    \n",
    "    # Save yearly theme counts\n",
    "    if 'yearly_theme_counts' in locals() and not yearly_theme_counts.empty:\n",
    "        yearly_theme_counts.to_csv(f'{RESULTS_PATH}/theme_counts_by_year.csv')\n",
    "    \n",
    "    # Create separate XLSX files for each theme (this is the new part)\n",
    "    for theme in themes.keys():\n",
    "        theme_papers = df[df['primary_theme'] == theme]\n",
    "        if len(theme_papers) > 0:\n",
    "            # Include more comprehensive information for literature review selection\n",
    "            output_columns = [\n",
    "                'Title', 'year', 'Authors', 'Abstract', \n",
    "                'Journal', 'Volume', 'Issue', 'DOI', 'URL'\n",
    "            ]\n",
    "            \n",
    "            # Only include columns that exist in the dataframe\n",
    "            available_columns = [col for col in output_columns if col in theme_papers.columns]\n",
    "            \n",
    "            # Create the filename\n",
    "            filename = f'{RESULTS_PATH}/{theme.replace(\" \", \"_\")}_papers.xlsx'\n",
    "            \n",
    "            # Export to Excel with enhanced formatting\n",
    "            with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "                theme_papers[available_columns].to_excel(writer, index=False, sheet_name=theme[:31])  # Excel sheet names have a 31 character limit\n",
    "                \n",
    "                # Get the workbook and the worksheet\n",
    "                workbook = writer.book\n",
    "                worksheet = writer.sheets[theme[:31]]\n",
    "                \n",
    "                # Format header row (bold, background color)\n",
    "                for col_num, value in enumerate(available_columns, 1):\n",
    "                    cell = worksheet.cell(row=1, column=col_num)\n",
    "                    cell.font = openpyxl.styles.Font(bold=True)\n",
    "                    cell.fill = openpyxl.styles.PatternFill(\n",
    "                        start_color='E6E6E6',\n",
    "                        end_color='E6E6E6',\n",
    "                        fill_type='solid'\n",
    "                    )\n",
    "                \n",
    "                # Adjust column widths for better readability\n",
    "                for i, column in enumerate(available_columns):\n",
    "                    if column == 'Title':\n",
    "                        worksheet.column_dimensions[openpyxl.utils.get_column_letter(i+1)].width = 40\n",
    "                    elif column == 'Abstract':\n",
    "                        worksheet.column_dimensions[openpyxl.utils.get_column_letter(i+1)].width = 60\n",
    "                    elif column == 'Authors':\n",
    "                        worksheet.column_dimensions[openpyxl.utils.get_column_letter(i+1)].width = 30\n",
    "                    else:\n",
    "                        worksheet.column_dimensions[openpyxl.utils.get_column_letter(i+1)].width = 15\n",
    "            \n",
    "            print(f\"Created {filename} with {len(theme_papers)} papers\")\n",
    "    \n",
    "    # Also create an \"Uncategorized\" file if needed\n",
    "    uncategorized_papers = df[df['primary_theme'] == \"Uncategorized\"]\n",
    "    if len(uncategorized_papers) > 0:\n",
    "        # Include more comprehensive information\n",
    "        output_columns = [\n",
    "            'Title', 'year', 'Authors', 'Abstract', \n",
    "            'Journal', 'Volume', 'Issue', 'DOI', 'URL'\n",
    "        ]\n",
    "        \n",
    "        # Only include columns that exist in the dataframe\n",
    "        available_columns = [col for col in output_columns if col in uncategorized_papers.columns]\n",
    "        \n",
    "        # Create the filename\n",
    "        filename = f'{RESULTS_PATH}/Uncategorized_papers.xlsx'\n",
    "        \n",
    "        # Export to Excel with enhanced formatting\n",
    "        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "            uncategorized_papers[available_columns].to_excel(writer, index=False, sheet_name='Uncategorized')\n",
    "            \n",
    "            # Get the workbook and the worksheet\n",
    "            workbook = writer.book\n",
    "            worksheet = writer.sheets['Uncategorized']\n",
    "            \n",
    "            # Format header row (bold, background color)\n",
    "            for col_num, value in enumerate(available_columns, 1):\n",
    "                cell = worksheet.cell(row=1, column=col_num)\n",
    "                cell.font = openpyxl.styles.Font(bold=True)\n",
    "                cell.fill = openpyxl.styles.PatternFill(\n",
    "                    start_color='E6E6E6',\n",
    "                    end_color='E6E6E6',\n",
    "                    fill_type='solid'\n",
    "                )\n",
    "            \n",
    "            # Adjust column widths for better readability\n",
    "            for i, column in enumerate(available_columns):\n",
    "                if column == 'Title':\n",
    "                    worksheet.column_dimensions[openpyxl.utils.get_column_letter(i+1)].width = 40\n",
    "                elif column == 'Abstract':\n",
    "                    worksheet.column_dimensions[openpyxl.utils.get_column_letter(i+1)].width = 60\n",
    "                elif column == 'Authors':\n",
    "                    worksheet.column_dimensions[openpyxl.utils.get_column_letter(i+1)].width = 30\n",
    "                else:\n",
    "                    worksheet.column_dimensions[openpyxl.utils.get_column_letter(i+1)].width = 15\n",
    "        \n",
    "        print(f\"Created {filename} with {len(uncategorized_papers)} papers\")\n",
    "    \n",
    "    print(\"Theme analysis complete with separate XLSX files for each theme.\")\n",
    "    return theme_df\n",
    "\n",
    "# Update the import statements at the beginning of the file to include openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import openpyxl  # Add this import for Excel formatting\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90140573-434f-4ce1-a099-051752276f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_papers_by_theme(df, themes):\n",
    "    \"\"\"\n",
    "    Categorize papers into predefined themes using keyword matching.\n",
    "    Returns the dataframe with a new 'theme' column.\n",
    "    \"\"\"\n",
    "    print(\"Categorizing papers into themes...\")\n",
    "    \n",
    "    # Ensure we have processed abstracts\n",
    "    if 'processed_abstract' not in df.columns:\n",
    "        df['processed_abstract'] = df['Abstract'].apply(preprocess_text)\n",
    "    \n",
    "    # Function to match document to themes\n",
    "    def categorize_document(abstract, title):\n",
    "        if pd.isna(abstract) and pd.isna(title):\n",
    "            return \"Uncategorized\"\n",
    "        \n",
    "        # Combine and preprocess text\n",
    "        text = ''\n",
    "        if not pd.isna(abstract):\n",
    "            text += abstract.lower() + ' '\n",
    "        if not pd.isna(title):\n",
    "            text += title.lower()\n",
    "        \n",
    "        # Count matches for each theme\n",
    "        theme_scores = {}\n",
    "        for theme_name, keywords in themes.items():\n",
    "            # Count occurrences of each keyword\n",
    "            score = sum(1 for keyword in keywords if keyword in text)\n",
    "            theme_scores[theme_name] = score\n",
    "        \n",
    "        # Assign primary theme (highest score)\n",
    "        if max(theme_scores.values()) > 0:\n",
    "            primary_theme = max(theme_scores.items(), key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            primary_theme = \"Uncategorized\"\n",
    "            \n",
    "        return primary_theme\n",
    "    \n",
    "    # Apply categorization\n",
    "    df['theme'] = df.apply(\n",
    "        lambda row: categorize_document(row['processed_abstract'], row['Title']), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Count papers by theme\n",
    "    theme_counts = df['theme'].value_counts()\n",
    "    \n",
    "    print(\"\\nPapers by theme:\")\n",
    "    for theme, count in theme_counts.items():\n",
    "        print(f\"  {theme}: {count} papers\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_theme_visualizations(df, results_path):\n",
    "    \"\"\"\n",
    "    Create visualizations for theme distribution.\n",
    "    \"\"\"\n",
    "    print(\"Creating theme visualizations...\")\n",
    "    \n",
    "    # Count papers by theme\n",
    "    theme_counts = df['theme'].value_counts()\n",
    "    theme_percent = (theme_counts / theme_counts.sum() * 100).round(1)\n",
    "    \n",
    "    # Create pie chart\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    colors = {'Management and Leadership': 'royalblue', \n",
    "              'Process Improvement': 'forestgreen',\n",
    "              'Continuous Learning': 'darkorange',\n",
    "              'Uncategorized': 'lightgray'}\n",
    "    \n",
    "    theme_colors = [colors.get(theme, 'lightgray') for theme in theme_counts.index]\n",
    "    wedges, texts, autotexts = plt.pie(\n",
    "        theme_counts, \n",
    "        labels=theme_counts.index, \n",
    "        autopct='%1.1f%%',\n",
    "        shadow=False, \n",
    "        startangle=90, \n",
    "        explode=[0.05] * len(theme_counts),\n",
    "        colors=theme_colors\n",
    "    )\n",
    "    \n",
    "    # Enhance text visibility\n",
    "    for text in texts:\n",
    "        text.set_fontsize(12)\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_fontsize(10)\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    \n",
    "    plt.title('Distribution of Papers by Theme', fontsize=16)\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{results_path}/theme_distribution_pie.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Create bar chart\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(\n",
    "        theme_counts.index,\n",
    "        theme_counts.values,\n",
    "        color=[colors.get(theme, 'lightgray') for theme in theme_counts.index]\n",
    "    )\n",
    "    \n",
    "    plt.title('Distribution of Papers by Theme', fontsize=16)\n",
    "    plt.xlabel('Theme', fontsize=14)\n",
    "    plt.ylabel('Number of Papers', fontsize=14)\n",
    "    plt.xticks(rotation=15, ha='right')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add data labels\n",
    "    for i, (count, percent) in enumerate(zip(theme_counts, theme_percent)):\n",
    "        plt.text(\n",
    "            i, \n",
    "            count + 0.5, \n",
    "            f\"{count} ({percent}%)\", \n",
    "            ha='center', \n",
    "            fontsize=10,\n",
    "            fontweight='bold'\n",
    "        )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{results_path}/theme_distribution_bar.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    return theme_counts, theme_percent\n",
    "\n",
    "def create_theme_word_clouds(df, themes, results_path):\n",
    "    \"\"\"\n",
    "    Generate word clouds for each theme.\n",
    "    \"\"\"\n",
    "    print(\"Creating theme word clouds...\")\n",
    "    \n",
    "    # Generate word clouds for each theme\n",
    "    for theme in themes.keys():\n",
    "        # Select papers for this theme\n",
    "        theme_papers = df[df['theme'] == theme]\n",
    "        \n",
    "        if len(theme_papers) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Combine all text for this theme\n",
    "        theme_text = ' '.join(theme_papers['processed_abstract'].dropna())\n",
    "        \n",
    "        if not theme_text.strip():\n",
    "            continue\n",
    "        \n",
    "        # Generate word cloud\n",
    "        wordcloud = WordCloud(\n",
    "            width=800, \n",
    "            height=400, \n",
    "            background_color='white',\n",
    "            max_words=100, \n",
    "            contour_width=3, \n",
    "            contour_color='steelblue'\n",
    "        ).generate(theme_text)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(f'Key Terms in {theme} Papers', fontsize=16)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{results_path}/wordcloud_{theme.replace(\" \", \"_\")}.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "def analyze_theme_trends(df, results_path):\n",
    "    \"\"\"\n",
    "    Analyze theme trends over time.\n",
    "    \"\"\"\n",
    "    print(\"Analyzing theme trends over time...\")\n",
    "    \n",
    "    try:\n",
    "        # Define colors\n",
    "        colors = {'Management and Leadership': 'royalblue', \n",
    "                'Process Improvement': 'forestgreen',\n",
    "                'Continuous Learning': 'darkorange',\n",
    "                'Uncategorized': 'lightgray'}\n",
    "        \n",
    "        # Group by year and theme\n",
    "        yearly_theme_counts = df.groupby(['year', 'theme']).size().unstack(fill_value=0)\n",
    "        \n",
    "        # Plot theme trends\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        ax = yearly_theme_counts.plot(\n",
    "            kind='line', \n",
    "            marker='o',\n",
    "            linewidth=2,\n",
    "            markersize=6,\n",
    "            ax=plt.gca(),\n",
    "            color=[colors.get(theme, 'lightgray') for theme in yearly_theme_counts.columns]\n",
    "        )\n",
    "        \n",
    "        plt.title('Evolution of Research Themes Over Time', fontsize=16)\n",
    "        plt.xlabel('Year', fontsize=14)\n",
    "        plt.ylabel('Number of Publications', fontsize=14)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.legend(title='Theme', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{results_path}/theme_trends_over_time.png', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Create stacked area chart\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        yearly_theme_counts.plot.area(\n",
    "            stacked=True, \n",
    "            alpha=0.7, \n",
    "            ax=plt.gca(),\n",
    "            color=[colors.get(theme, 'lightgray') for theme in yearly_theme_counts.columns]\n",
    "        )\n",
    "        plt.title('Cumulative Growth of Research Themes', fontsize=16)\n",
    "        plt.xlabel('Year', fontsize=14)\n",
    "        plt.ylabel('Number of Publications', fontsize=14)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.legend(title='Theme', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{results_path}/theme_cumulative_growth.png', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Calculate percentage distribution over time\n",
    "        yearly_percentages = yearly_theme_counts.div(yearly_theme_counts.sum(axis=1), axis=0) * 100\n",
    "        \n",
    "        plt.figure(figsize=(14, 8))\n",
    "        yearly_percentages.plot.area(\n",
    "            stacked=True, \n",
    "            alpha=0.7, \n",
    "            ax=plt.gca(),\n",
    "            color=[colors.get(theme, 'lightgray') for theme in yearly_percentages.columns]\n",
    "        )\n",
    "        plt.title('Relative Proportion of Research Themes Over Time', fontsize=16)\n",
    "        plt.xlabel('Year', fontsize=14)\n",
    "        plt.ylabel('Percentage of Publications', fontsize=14)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.legend(title='Theme', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{results_path}/theme_percentage_over_time.png', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Save yearly theme counts\n",
    "        yearly_theme_counts.to_csv(f'{results_path}/theme_counts_by_year.csv')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in time trend analysis: {e}\")\n",
    "\n",
    "def export_themed_excel_files(df, results_path):\n",
    "    \"\"\"\n",
    "    Export separate Excel files for each theme using the original dataframe.\n",
    "    This preserves all columns including URLs and other metadata.\n",
    "    \"\"\"\n",
    "    print(\"Exporting themed Excel files...\")\n",
    "    \n",
    "    # Get unique themes\n",
    "    themes = df['theme'].unique()\n",
    "    \n",
    "    for theme in themes:\n",
    "        # Filter data for this theme\n",
    "        theme_papers = df[df['theme'] == theme]\n",
    "        \n",
    "        if len(theme_papers) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Create filename with sanitized theme name\n",
    "        filename = f'{results_path}/{theme.replace(\" \", \"_\")}_papers.xlsx'\n",
    "        \n",
    "        # Export to Excel with enhanced formatting\n",
    "        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "            # Export all columns from the original data\n",
    "            theme_papers.to_excel(writer, index=False, sheet_name=theme[:31])  # Excel sheet names have 31 char limit\n",
    "            \n",
    "            # Get the workbook and worksheet\n",
    "            workbook = writer.book\n",
    "            worksheet = writer.sheets[theme[:31]]\n",
    "            \n",
    "            # Format header row\n",
    "            for col_num, column_name in enumerate(theme_papers.columns, 1):\n",
    "                cell = worksheet.cell(row=1, column=col_num)\n",
    "                cell.font = Font(bold=True)\n",
    "                cell.fill = PatternFill(\n",
    "                    start_color='E6E6E6',\n",
    "                    end_color='E6E6E6',\n",
    "                    fill_type='solid'\n",
    "                )\n",
    "            \n",
    "            # Adjust column widths\n",
    "            for i, column in enumerate(theme_papers.columns):\n",
    "                col_letter = get_column_letter(i+1)\n",
    "                if column in ['Title', 'Abstract']:\n",
    "                    worksheet.column_dimensions[col_letter].width = 50\n",
    "                elif column == 'Authors':\n",
    "                    worksheet.column_dimensions[col_letter].width = 30\n",
    "                elif column in ['URL', 'DOI']:\n",
    "                    worksheet.column_dimensions[col_letter].width = 40\n",
    "                else:\n",
    "                    worksheet.column_dimensions[col_letter].width = 15\n",
    "        \n",
    "        print(f\"Created {filename} with {len(theme_papers)} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc2e096d-eddd-4741-84e8-3bc8dc997d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   Categorize papers into the three main themes identified from preliminary analysis:\n",
    "    1. Management and Leadership\n",
    "    2. Process Improvement\n",
    "    3. Continuous Learning (Data, Hansei)\n",
    "\"\"\"\n",
    "themes = {\n",
    "    'Management and Leadership': [\n",
    "        'management', 'leadership', 'project', 'program', 'service', 'officer', \n",
    "        'employee', 'organization', 'leader', 'command', 'government', 'agency',\n",
    "        'strategic', 'executive', 'administrative', 'planning', 'manager', 'performance',\n",
    "        'organizational', 'effectiveness', 'decision'\n",
    "    ],\n",
    "    \n",
    "    'Process Improvement': [\n",
    "        'process', 'improvement', 'system', 'model', 'lean', 'sigma', 'six', 'lss', \n",
    "        'waste', 'quality', 'efficiency', 'performance', 'optimization', 'methodology',\n",
    "        'streamline', 'workflow', 'operation', 'engineering', 'design', 'tool', 'technique',\n",
    "        'kaizen', 'value', 'stream', 'mapping', 'dmaic'\n",
    "    ],\n",
    "    \n",
    "    'Continuous Learning': [\n",
    "        'data', 'analysis', 'learning', 'continuous', 'hansei', 'improvement', 'training',\n",
    "        'education', 'knowledge', 'metric', 'measure', 'assessment', 'indicator', 'evaluation',\n",
    "        'analytics', 'insights', 'information', 'skill', 'competency', 'development', 'innovation',\n",
    "        'feedback', 'culture', 'adaptation', 'intelligence'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "222fd86d-1402-4e32-a2cc-22e1b3f4bf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorizing papers into the three main themes...\n",
      "\n",
      "Papers by theme:\n",
      "  Management and Leadership: 251 papers (54.0%)\n",
      "  Process Improvement: 167 papers (35.9%)\n",
      "  Continuous Learning: 43 papers (9.2%)\n",
      "  Uncategorized: 4 papers (0.9%)\n",
      "Created results/Management_and_Leadership_papers.xlsx with 251 papers\n",
      "Created results/Process_Improvement_papers.xlsx with 167 papers\n",
      "Created results/Continuous_Learning_papers.xlsx with 43 papers\n",
      "Created results/Uncategorized_papers.xlsx with 4 papers\n",
      "Theme analysis complete with separate XLSX files for each theme.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Theme</th>\n",
       "      <th>Count</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Management and Leadership</td>\n",
       "      <td>251</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Process Improvement</td>\n",
       "      <td>167</td>\n",
       "      <td>35.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Continuous Learning</td>\n",
       "      <td>43</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>4</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Theme  Count  Percentage\n",
       "0  Management and Leadership    251        54.0\n",
       "1        Process Improvement    167        35.9\n",
       "2        Continuous Learning     43         9.2\n",
       "3              Uncategorized      4         0.9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = categorize_papers_by_theme(df, themes)\n",
    "\n",
    "\n",
    "theme_counts, theme_percent = create_theme_visualizations(df, results_path)\n",
    "\n",
    "\n",
    "create_theme_word_clouds(df, themes, results_path)\n",
    "\n",
    "\n",
    "analyze_theme_trends(df, results_path)\n",
    "\n",
    "\n",
    "export_themed_excel_files(df, results_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
